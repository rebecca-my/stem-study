{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dot\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from data_parser import DataParser\n",
    "# from skip_gram import SkipGramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rsciagli/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (13,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "courses = pd.read_csv('/Users/rsciagli/documents/Fall2020/BAR/STU_CRS_TBL_full.csv', encoding='latin', dtype={'CRS_ID':str})\n",
    "#courses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# courses['CRS_ID'].isnull().sum()\n",
    "# courses['CRS_ID'].astype(float).isnull().sum()\n",
    "# courses[['CRS_ID','EARNED_BFORE_COHORT']]\n",
    "# for dummy, df in courses.groupby('EARNED_BFORE_COHORT')['CRS_ID']:\n",
    "#     print(dummy)\n",
    "#     print(len(df))\n",
    "#     display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#courses_sample = courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses['crs_ofcl_grd_nbr'.upper()].unique()\n",
    "mask_type = courses['CRS_TYPE']=='ENRL'\n",
    "discarded_grades = ['ZZ']\n",
    "discarded_transfer_crs = ['Y']                          \n",
    "mask_grade = ~courses['CRS_OFCL_GRD_CD'].isin(discarded_grades)                          \n",
    "mask = mask_grade&mask_type\n",
    "crs_embed_subset = courses[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "crs_df = pd.DataFrame(crs_embed_subset)\n",
    "crs_df['agg_id'] = crs_df['CRS_ID'].astype(str)\n",
    "print(crs_df['agg_id'].isnull().values.any())\n",
    "print(crs_df['CRS_ID'].astype(str).isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_by_id = crs_df['agg_id'].value_counts()\n",
    "big_subj_by_id = subj_by_id[subj_by_id>10].index\n",
    "crs_df['agg_id'] = np.where(crs_df['agg_id'].isin(big_subj_by_id), crs_df['agg_id'],'not_in_final_results')\n",
    "#pd.Series(pruned_id).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.hist(courses['CRS_SUBJ_DEPT_CD'].value_counts(), log=True, bins=[0,10,20,30,40,50,60,70,80,90,100])\n",
    "orphan_classes = crs_df[crs_df['agg_id']=='not_in_final_results']\n",
    "crs_dept_cd = orphan_classes['CRS_SUBJ_DEPT_CD'].value_counts()\n",
    "big_dept = crs_dept_cd[crs_dept_cd>10].index\n",
    "crs_df['agg_id'] = np.where(crs_df['agg_id'] != 'not_in_final_results', crs_df['agg_id'],\n",
    "                                        np.where(crs_df['CRS_SUBJ_DEPT_CD'].isin(big_dept),\n",
    "                                        crs_df['CRS_SUBJ_DEPT_CD'], 'other_dept'))    \n",
    "pd.Series(crs_df['agg_id']).value_counts()\n",
    "pd.Series(crs_df['agg_id']).value_counts()['other_dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(crs_df['agg_id'].isnull().values.any())\n",
    "#pd.Series(pruned_dept).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0., 997., 662., 490., 326., 298., 243., 222., 201., 183.]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(crs_df['agg_id'].value_counts(), log=True, bins=[0,10,20,30,40,50,60,70,80,90,100])\n",
    "# print(crs_df['agg_id'].nunique())\n",
    "# print(crs_df['agg_id'].count())\n",
    "# print(crs_df['agg_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MUS',\n",
       " 'HER',\n",
       " 'ENG',\n",
       " 'HIST',\n",
       " 'other_dept',\n",
       " 'CSCI',\n",
       " 'BUS',\n",
       " 'RADI',\n",
       " 'SOC',\n",
       " 'REL',\n",
       " 'BIOL',\n",
       " 'CEUS',\n",
       " 'PSY',\n",
       " 'THTR',\n",
       " 'ANTH',\n",
       " 'FINA',\n",
       " 'NEWM',\n",
       " 'MATH',\n",
       " 'HPER',\n",
       " 'EDUC',\n",
       " 'VCD',\n",
       " 'SPEA',\n",
       " 'SPAN',\n",
       " 'PHIL',\n",
       " 'OVST',\n",
       " 'RADS',\n",
       " 'COMM',\n",
       " 'SLAV',\n",
       " 'AHLT',\n",
       " 'PBHL',\n",
       " 'HIA',\n",
       " 'GEOG',\n",
       " 'PPOL',\n",
       " 'PHYS',\n",
       " 'POLS',\n",
       " 'GER',\n",
       " 'HIM',\n",
       " 'FREN',\n",
       " 'MET',\n",
       " 'INFO',\n",
       " 'SPCH',\n",
       " 'ECE',\n",
       " 'BME',\n",
       " 'JOUR',\n",
       " 'CIT',\n",
       " 'INTR',\n",
       " 'INST',\n",
       " 'GEOL',\n",
       " 'HISP',\n",
       " 'CLAS',\n",
       " 'PATH',\n",
       " 'CGT',\n",
       " 'NURS',\n",
       " 'HSC',\n",
       " 'NELC',\n",
       " 'EALC',\n",
       " 'CJUS',\n",
       " 'SPH',\n",
       " 'ME',\n",
       " 'ECON',\n",
       " 'CEMT',\n",
       " 'CHEM',\n",
       " 'LING',\n",
       " 'AAAD',\n",
       " 'ECET',\n",
       " 'CJHS',\n",
       " 'SHRS',\n",
       " 'PAHM',\n",
       " 'OLS',\n",
       " 'AFRO',\n",
       " 'KINE',\n",
       " 'CMLT',\n",
       " 'PHST',\n",
       " 'ASL',\n",
       " 'TEL',\n",
       " 'ENGR',\n",
       " 'RADX',\n",
       " 'EMER',\n",
       " 'FOLK',\n",
       " 'JSTU',\n",
       " 'MA',\n",
       " 'WGS',\n",
       " 'COM',\n",
       " 'MSCH',\n",
       " 'HON',\n",
       " 'LTAM',\n",
       " 'SOAD',\n",
       " 'NMAT',\n",
       " 'DHYG',\n",
       " 'ITAL',\n",
       " 'FILM',\n",
       " 'HSRV',\n",
       " 'MIL',\n",
       " 'SPHS',\n",
       " 'HPSC',\n",
       " 'INTL',\n",
       " 'DAST',\n",
       " 'COAS',\n",
       " 'TCM',\n",
       " 'SEAS',\n",
       " 'AFRI',\n",
       " 'WOST',\n",
       " 'HSS',\n",
       " 'NMCM',\n",
       " 'MSTE',\n",
       " 'STAT',\n",
       " 'CMCL',\n",
       " 'COLL',\n",
       " 'AHSC',\n",
       " 'ART',\n",
       " 'CHRI',\n",
       " 'AMST',\n",
       " 'RAON',\n",
       " 'TCEM',\n",
       " 'LSTU',\n",
       " 'TECH',\n",
       " 'HTM',\n",
       " 'DPIS',\n",
       " 'WEUR',\n",
       " 'SUST',\n",
       " 'PLSC',\n",
       " 'AHST',\n",
       " 'FIS',\n",
       " 'CHM',\n",
       " 'CS',\n",
       " 'SCI',\n",
       " 'UCOL',\n",
       " 'DANC',\n",
       " 'PIE',\n",
       " 'SWK',\n",
       " 'TESM',\n",
       " 'MSL',\n",
       " 'MLS',\n",
       " 'INMS',\n",
       " 'IET',\n",
       " 'MSTD',\n",
       " 'ILCS',\n",
       " 'FRIT',\n",
       " 'EEN',\n",
       " 'PHSL',\n",
       " 'SLA',\n",
       " 'GNDR',\n",
       " 'MSCI',\n",
       " 'BMET',\n",
       " 'ARTH',\n",
       " 'AAST',\n",
       " 'LATS',\n",
       " 'SA',\n",
       " 'EAS',\n",
       " 'MHHS']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def helper(x):\n",
    "    output = x\n",
    "    try:\n",
    "        int(x)\n",
    "        output = None\n",
    "    except:\n",
    "        pass\n",
    "    return output\n",
    "\n",
    "[x for x in list(map(helper, crs_df['agg_id'].value_counts().index)) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_id = list(crs_df['agg_id'].unique())\n",
    "course_to_id = dict([(name, i) for i, name in enumerate(embedding_id)])\n",
    "\n",
    "def make_set(df):\n",
    "    return set(df['agg_id'].map(course_to_id))\n",
    "\n",
    "#dummy = crs_df.groupby(['PRSN_UNIV_ID','ACAD_TERM_CD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(embedding_id).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dummy = crs_df.groupby(['PRSN_UNIV_ID','ACAD_TERM_CD'])\n",
    "# len(dummy)\n",
    "agg_course_to_dept = dict(zip(crs_df['agg_id'],crs_df['CRS_SUBJ_DEPT_CD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments = [agg_course_to_dept[identifier] for identifier in embedding_id]\n",
    "#agg_course_to_dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_students = list(crs_df['PRSN_UNIV_ID'].unique())\n",
    "#pd.Series(unique_students).unique()\n",
    "#pd.Series(unique_students).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_students = crs_df['PRSN_UNIV_ID'].unique()\n",
    "np.random.seed(9)\n",
    "np.random.shuffle(unique_students)\n",
    "n_train = int(0.1*len(unique_students))\n",
    "n_valid = int(0.02*len(unique_students))\n",
    "train_students = unique_students[:n_train]\n",
    "valid_students = unique_students[n_train:n_train+n_valid]\n",
    "crs_df_train = crs_df[crs_df['PRSN_UNIV_ID'].isin(train_students)]\n",
    "crs_df_valid = crs_df[crs_df['PRSN_UNIV_ID'].isin(valid_students)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89584\n",
      "12889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128899"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(crs_df_train.groupby(['PRSN_UNIV_ID','ACAD_TERM_CD'])))\n",
    "print(len(train_students))\n",
    "len(unique_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2577"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(crs_df_valid.groupby(['PRSN_UNIV_ID','ACAD_TERM_CD'])))\n",
    "len(valid_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(): \n",
    "    negative_courses = crs_df['agg_id'].map(course_to_id)\n",
    "    n_neg = len(negative_courses)\n",
    "    while True:\n",
    "        for (student, term), df in crs_df_train.groupby(['PRSN_UNIV_ID','ACAD_TERM_CD']):\n",
    "            courses_set = make_set(df)\n",
    "            if len(courses_set) > 1:\n",
    "                for crs_1 in courses_set:\n",
    "                    contexts = []\n",
    "                    courses_x = []\n",
    "                    matches = []\n",
    "                    for crs_2 in courses_set: \n",
    "                        x = crs_1\n",
    "                        y = crs_2\n",
    "                        if x!=y:\n",
    "                            context = list(negative_courses.iloc[np.random.choice(n_neg,4)]) + [y]\n",
    "                            course = 5*[x]\n",
    "                            match = [0,0,0,0,1]\n",
    "                            contexts.append(np.array(context).reshape(5,1))\n",
    "                            courses_x.append(np.array(course).reshape(5,1))\n",
    "                            matches.append(np.array(match).reshape(5,1))\n",
    "                    contexts = np.concatenate(contexts, axis=0)\n",
    "                    courses_x = np.concatenate(courses_x, axis=0)\n",
    "                    matches = np.concatenate(matches, axis=0)\n",
    "                    yield [contexts, courses_x], matches\n",
    "                \n",
    "def valid_generator(): \n",
    "    negative_courses = crs_df['agg_id'].map(course_to_id)\n",
    "    n_neg = len(negative_courses)\n",
    "    while True:\n",
    "        for (student, term), df in crs_df_valid.groupby(['PRSN_UNIV_ID','ACAD_TERM_CD']):\n",
    "            courses_set = make_set(df)\n",
    "            if len(courses_set) > 1:\n",
    "                for crs_1 in courses_set:\n",
    "                    contexts = []\n",
    "                    courses_x = []\n",
    "                    matches = []\n",
    "                    for crs_2 in courses_set: \n",
    "                        x = crs_1\n",
    "                        y = crs_2\n",
    "                        if x!=y:\n",
    "                            context = list(negative_courses.iloc[np.random.choice(n_neg,4)]) + [y]\n",
    "                            course = 5*[x]\n",
    "                            match = [0,0,0,0,1]\n",
    "                            contexts.append(np.array(context).reshape(5,1))\n",
    "                            courses_x.append(np.array(course).reshape(5,1))\n",
    "                            matches.append(np.array(match).reshape(5,1))\n",
    "                    contexts = np.concatenate(contexts, axis=0)\n",
    "                    courses_x = np.concatenate(courses_x, axis=0)\n",
    "                    matches = np.concatenate(matches, axis=0)\n",
    "                    yield [contexts, courses_x], matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Course_ids (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "contxt (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Course_embedding (Embedding)    (None, 1, 218)       1583770     Course_ids[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Context_embedding (Embedding)   (None, 1, 218)       1583770     contxt[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           Course_embedding[0][0]           \n",
      "                                                                 Context_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid (TensorFlow [(None, 1, 1)]       0           dot[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 3,167,540\n",
      "Trainable params: 3,167,540\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 218\n",
    "optimizer = Adagrad(lr=0.02879)\n",
    "\n",
    "# def _build_model(course_input):\n",
    "#     input_ = Input(shape=x.shape[1:], name='Course ids')\n",
    "#     embed = Embedding(len(embedding_id), embed_dim, name='Course embedding')(input_)\n",
    "#     output = Dense(len(embedding_id), activation='softmax', name='Course probabilities')(embed) \n",
    "    \n",
    "#     model = Model(inputs=input_,outputs=output, name='Model')\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "#     return model\n",
    "\n",
    "input_course_ = Input(shape=(1,), name='Course_ids')\n",
    "input_context_ = Input(shape=(1,), name='contxt')\n",
    "embed = Embedding(len(embedding_id), embed_dim, name='Course_embedding')(input_course_)\n",
    "embed2 = Embedding(len(embedding_id), embed_dim, name='Context_embedding')(input_context_)\n",
    "output = Dot(-1)([embed, embed2])\n",
    "sigmoid = keras.activations.sigmoid(output)\n",
    "model = Model(inputs=[input_course_, input_context_],outputs=sigmoid, name='Model')\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 101681 steps, validate for 25421 steps\n",
      "Epoch 1/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.6721 - acc: 0.6409\n",
      "Epoch 00001: val_loss improved from inf to 0.61794, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 1200s 12ms/step - loss: 0.6721 - acc: 0.6409 - val_loss: 0.6179 - val_acc: 0.7643\n",
      "Epoch 2/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.7867\n",
      "Epoch 00002: val_loss improved from 0.61794 to 0.54293, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 1052s 10ms/step - loss: 0.5709 - acc: 0.7867 - val_loss: 0.5429 - val_acc: 0.7957\n",
      "Epoch 3/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.8021\n",
      "Epoch 00003: val_loss improved from 0.54293 to 0.49154, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 1058s 10ms/step - loss: 0.5203 - acc: 0.8021 - val_loss: 0.4915 - val_acc: 0.8098\n",
      "Epoch 4/150\n",
      "101673/101681 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.8160\n",
      "Epoch 00004: val_loss did not improve from 0.49154\n",
      "101681/101681 [==============================] - 1137s 11ms/step - loss: 0.4668 - acc: 0.8160 - val_loss: 0.5002 - val_acc: 0.8056\n",
      "Epoch 5/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.4673 - acc: 0.8118\n",
      "Epoch 00005: val_loss improved from 0.49154 to 0.46710, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 1052s 10ms/step - loss: 0.4673 - acc: 0.8118 - val_loss: 0.4671 - val_acc: 0.8101\n",
      "Epoch 6/150\n",
      "101671/101681 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8141\n",
      "Epoch 00006: val_loss did not improve from 0.46710\n",
      "101681/101681 [==============================] - 1058s 10ms/step - loss: 0.4504 - acc: 0.8141 - val_loss: 0.4711 - val_acc: 0.8131\n",
      "Epoch 7/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8177\n",
      "Epoch 00007: val_loss improved from 0.46710 to 0.44279, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 949s 9ms/step - loss: 0.4429 - acc: 0.8177 - val_loss: 0.4428 - val_acc: 0.8222\n",
      "Epoch 8/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8286\n",
      "Epoch 00008: val_loss did not improve from 0.44279\n",
      "101681/101681 [==============================] - 945s 9ms/step - loss: 0.4161 - acc: 0.8286 - val_loss: 0.4827 - val_acc: 0.8086\n",
      "Epoch 9/150\n",
      "101673/101681 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8265\n",
      "Epoch 00009: val_loss improved from 0.44279 to 0.43469, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 928s 9ms/step - loss: 0.4186 - acc: 0.8265 - val_loss: 0.4347 - val_acc: 0.8204\n",
      "Epoch 10/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8238\n",
      "Epoch 00010: val_loss did not improve from 0.43469\n",
      "101681/101681 [==============================] - 932s 9ms/step - loss: 0.4185 - acc: 0.8238 - val_loss: 0.4549 - val_acc: 0.8177\n",
      "Epoch 11/150\n",
      "101672/101681 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8246\n",
      "Epoch 00011: val_loss improved from 0.43469 to 0.43292, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 961s 9ms/step - loss: 0.4165 - acc: 0.8246 - val_loss: 0.4329 - val_acc: 0.8198\n",
      "Epoch 12/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8334\n",
      "Epoch 00012: val_loss did not improve from 0.43292\n",
      "101681/101681 [==============================] - 956s 9ms/step - loss: 0.3988 - acc: 0.8334 - val_loss: 0.4382 - val_acc: 0.8199\n",
      "Epoch 13/150\n",
      "101672/101681 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8374\n",
      "Epoch 00013: val_loss improved from 0.43292 to 0.40502, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 944s 9ms/step - loss: 0.3838 - acc: 0.8374 - val_loss: 0.4050 - val_acc: 0.8316\n",
      "Epoch 14/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8273\n",
      "Epoch 00014: val_loss did not improve from 0.40502\n",
      "101681/101681 [==============================] - 907s 9ms/step - loss: 0.4110 - acc: 0.8273 - val_loss: 0.4318 - val_acc: 0.8220\n",
      "Epoch 15/150\n",
      "101672/101681 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8296\n",
      "Epoch 00015: val_loss improved from 0.40502 to 0.40148, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 930s 9ms/step - loss: 0.3995 - acc: 0.8296 - val_loss: 0.4015 - val_acc: 0.8289\n",
      "Epoch 16/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8343\n",
      "Epoch 00016: val_loss improved from 0.40148 to 0.39914, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 943s 9ms/step - loss: 0.3914 - acc: 0.8343 - val_loss: 0.3991 - val_acc: 0.8316\n",
      "Epoch 17/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8434\n",
      "Epoch 00017: val_loss improved from 0.39914 to 0.37212, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 928s 9ms/step - loss: 0.3704 - acc: 0.8434 - val_loss: 0.3721 - val_acc: 0.8431\n",
      "Epoch 18/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8324\n",
      "Epoch 00018: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 897s 9ms/step - loss: 0.3947 - acc: 0.8324 - val_loss: 0.4070 - val_acc: 0.8267\n",
      "Epoch 19/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.3888 - acc: 0.8329\n",
      "Epoch 00019: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 916s 9ms/step - loss: 0.3888 - acc: 0.8329 - val_loss: 0.3904 - val_acc: 0.8343\n",
      "Epoch 20/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8345\n",
      "Epoch 00020: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 913s 9ms/step - loss: 0.3861 - acc: 0.8345 - val_loss: 0.3796 - val_acc: 0.8402\n",
      "Epoch 21/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8442\n",
      "Epoch 00021: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 905s 9ms/step - loss: 0.3661 - acc: 0.8442 - val_loss: 0.3886 - val_acc: 0.8357\n",
      "Epoch 22/150\n",
      "101673/101681 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8393\n",
      "Epoch 00022: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 885s 9ms/step - loss: 0.3780 - acc: 0.8393 - val_loss: 0.3983 - val_acc: 0.8328\n",
      "Epoch 23/150\n",
      "101671/101681 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8359\n",
      "Epoch 00023: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 896s 9ms/step - loss: 0.3806 - acc: 0.8359 - val_loss: 0.3955 - val_acc: 0.8336\n",
      "Epoch 24/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8351\n",
      "Epoch 00024: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 911s 9ms/step - loss: 0.3810 - acc: 0.8351 - val_loss: 0.3859 - val_acc: 0.8396\n",
      "Epoch 25/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8438\n",
      "Epoch 00025: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 909s 9ms/step - loss: 0.3653 - acc: 0.8438 - val_loss: 0.4117 - val_acc: 0.8269\n",
      "Epoch 26/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8455\n",
      "Epoch 00026: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 910s 9ms/step - loss: 0.3591 - acc: 0.8455 - val_loss: 0.4113 - val_acc: 0.8301\n",
      "Epoch 27/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8369\n",
      "Epoch 00027: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 853s 8ms/step - loss: 0.3805 - acc: 0.8369 - val_loss: 0.3916 - val_acc: 0.8348\n",
      "Epoch 28/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8375\n",
      "Epoch 00028: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 853s 8ms/step - loss: 0.3743 - acc: 0.8375 - val_loss: 0.4014 - val_acc: 0.8306\n",
      "Epoch 29/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8420\n",
      "Epoch 00029: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 864s 8ms/step - loss: 0.3663 - acc: 0.8420 - val_loss: 0.3947 - val_acc: 0.8342\n",
      "Epoch 30/150\n",
      "101675/101681 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8507\n",
      "Epoch 00030: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 860s 8ms/step - loss: 0.3492 - acc: 0.8507 - val_loss: 0.3806 - val_acc: 0.8389\n",
      "Epoch 31/150\n",
      "101673/101681 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8382\n",
      "Epoch 00031: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 844s 8ms/step - loss: 0.3748 - acc: 0.8382 - val_loss: 0.3812 - val_acc: 0.8379\n",
      "Epoch 32/150\n",
      "101674/101681 [============================>.] - ETA: 0s - loss: 0.3695 - acc: 0.8392\n",
      "Epoch 00032: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 851s 8ms/step - loss: 0.3695 - acc: 0.8392 - val_loss: 0.3816 - val_acc: 0.8360\n",
      "Epoch 33/150\n",
      "101672/101681 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8411\n",
      "Epoch 00033: val_loss did not improve from 0.37212\n",
      "101681/101681 [==============================] - 882s 9ms/step - loss: 0.3661 - acc: 0.8411 - val_loss: 0.3765 - val_acc: 0.8391\n",
      "Epoch 34/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8498\n",
      "Epoch 00034: val_loss improved from 0.37212 to 0.35600, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 942s 9ms/step - loss: 0.3490 - acc: 0.8498 - val_loss: 0.3560 - val_acc: 0.8482\n",
      "Epoch 35/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8442\n",
      "Epoch 00035: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 860s 8ms/step - loss: 0.3621 - acc: 0.8442 - val_loss: 0.3780 - val_acc: 0.8381\n",
      "Epoch 36/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8407\n",
      "Epoch 00036: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 927s 9ms/step - loss: 0.3653 - acc: 0.8407 - val_loss: 0.3689 - val_acc: 0.8427\n",
      "Epoch 37/150\n",
      "101678/101681 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8405\n",
      "Epoch 00037: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 907s 9ms/step - loss: 0.3655 - acc: 0.8405 - val_loss: 0.3663 - val_acc: 0.8444\n",
      "Epoch 38/150\n",
      "101675/101681 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8489\n",
      "Epoch 00038: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 903s 9ms/step - loss: 0.3499 - acc: 0.8489 - val_loss: 0.3567 - val_acc: 0.8476\n",
      "Epoch 39/150\n",
      "101670/101681 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8494\n",
      "Epoch 00039: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 878s 9ms/step - loss: 0.3482 - acc: 0.8494 - val_loss: 0.4013 - val_acc: 0.8323\n",
      "Epoch 40/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8413\n",
      "Epoch 00040: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 857s 8ms/step - loss: 0.3657 - acc: 0.8413 - val_loss: 0.3740 - val_acc: 0.8405\n",
      "Epoch 41/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8418\n",
      "Epoch 00041: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 875s 9ms/step - loss: 0.3617 - acc: 0.8418 - val_loss: 0.3778 - val_acc: 0.8402\n",
      "Epoch 42/150\n",
      "101675/101681 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8471- ETA: 0s - loss: 0.3525 - ac\n",
      "Epoch 00042: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 880s 9ms/step - loss: 0.3524 - acc: 0.8471 - val_loss: 0.3795 - val_acc: 0.8383\n",
      "Epoch 43/150\n",
      "101671/101681 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8541\n",
      "Epoch 00043: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 1015s 10ms/step - loss: 0.3388 - acc: 0.8541 - val_loss: 0.3999 - val_acc: 0.8333\n",
      "Epoch 44/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8415\n",
      "Epoch 00044: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 1264s 12ms/step - loss: 0.3636 - acc: 0.8415 - val_loss: 0.3697 - val_acc: 0.8418\n",
      "Epoch 45/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8430- - ETA: 0s - loss: 0.3582 - acc:\n",
      "Epoch 00045: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 2125s 21ms/step - loss: 0.3582 - acc: 0.8430 - val_loss: 0.3872 - val_acc: 0.8355\n",
      "Epoch 46/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8457\n",
      "Epoch 00046: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 2298s 23ms/step - loss: 0.3544 - acc: 0.8457 - val_loss: 0.3764 - val_acc: 0.8396\n",
      "Epoch 47/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8534\n",
      "Epoch 00047: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 2159s 21ms/step - loss: 0.3387 - acc: 0.8534 - val_loss: 0.3762 - val_acc: 0.8402\n",
      "Epoch 48/150\n",
      "101678/101681 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8464\n",
      "Epoch 00048: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 1375s 14ms/step - loss: 0.3541 - acc: 0.8464 - val_loss: 0.3568 - val_acc: 0.8486\n",
      "Epoch 49/150\n",
      "101677/101681 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8444\n",
      "Epoch 00049: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 953s 9ms/step - loss: 0.3550 - acc: 0.8444 - val_loss: 0.3779 - val_acc: 0.8375\n",
      "Epoch 50/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8442\n",
      "Epoch 00050: val_loss did not improve from 0.35600\n",
      "101681/101681 [==============================] - 1037s 10ms/step - loss: 0.3559 - acc: 0.8442 - val_loss: 0.3665 - val_acc: 0.8431\n",
      "Epoch 51/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8523\n",
      "Epoch 00051: val_loss improved from 0.35600 to 0.35230, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 1013s 10ms/step - loss: 0.3401 - acc: 0.8523 - val_loss: 0.3523 - val_acc: 0.8499\n",
      "Epoch 52/150\n",
      "101680/101681 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8513\n",
      "Epoch 00052: val_loss did not improve from 0.35230\n",
      "101681/101681 [==============================] - 919s 9ms/step - loss: 0.3423 - acc: 0.8513 - val_loss: 0.3527 - val_acc: 0.8486\n",
      "Epoch 53/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8447\n",
      "Epoch 00053: val_loss did not improve from 0.35230\n",
      "101681/101681 [==============================] - 911s 9ms/step - loss: 0.3556 - acc: 0.8447 - val_loss: 0.3619 - val_acc: 0.8449\n",
      "Epoch 54/150\n",
      "101676/101681 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8445\n",
      "Epoch 00054: val_loss did not improve from 0.35230\n",
      "101681/101681 [==============================] - 933s 9ms/step - loss: 0.3533 - acc: 0.8445 - val_loss: 0.3627 - val_acc: 0.8447\n",
      "Epoch 55/150\n",
      "101675/101681 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8506\n",
      "Epoch 00055: val_loss improved from 0.35230 to 0.34809, saving model to skip_gram_weights/skip_gram_model01.hdf5\n",
      "101681/101681 [==============================] - 939s 9ms/step - loss: 0.3434 - acc: 0.8506 - val_loss: 0.3481 - val_acc: 0.8521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "101672/101681 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8561\n",
      "Epoch 00056: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 866s 9ms/step - loss: 0.3325 - acc: 0.8561 - val_loss: 0.3834 - val_acc: 0.8374\n",
      "Epoch 57/150\n",
      "101673/101681 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8438\n",
      "Epoch 00057: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 874s 9ms/step - loss: 0.3560 - acc: 0.8438 - val_loss: 0.3672 - val_acc: 0.8437\n",
      "Epoch 58/150\n",
      "101679/101681 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8453\n",
      "Epoch 00058: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 875s 9ms/step - loss: 0.3512 - acc: 0.8453 - val_loss: 0.3671 - val_acc: 0.8434\n",
      "Epoch 59/150\n",
      "101675/101681 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.8485\n",
      "Epoch 00059: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 888s 9ms/step - loss: 0.3465 - acc: 0.8485 - val_loss: 0.3653 - val_acc: 0.8439\n",
      "Epoch 60/150\n",
      "101673/101681 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8556\n",
      "Epoch 00060: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 883s 9ms/step - loss: 0.3321 - acc: 0.8556 - val_loss: 0.3856 - val_acc: 0.8378\n",
      "Epoch 61/150\n",
      "101675/101681 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8481\n",
      "Epoch 00061: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 891s 9ms/step - loss: 0.3480 - acc: 0.8481 - val_loss: 0.3638 - val_acc: 0.8446\n",
      "Epoch 62/150\n",
      "101676/101681 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8470\n",
      "Epoch 00062: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 880s 9ms/step - loss: 0.3478 - acc: 0.8470 - val_loss: 0.3694 - val_acc: 0.8421\n",
      "Epoch 63/150\n",
      "101678/101681 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8462\n",
      "Epoch 00063: val_loss did not improve from 0.34809\n",
      "101681/101681 [==============================] - 894s 9ms/step - loss: 0.3496 - acc: 0.8462 - val_loss: 0.3717 - val_acc: 0.8405\n",
      "Epoch 64/150\n",
      " 43471/101681 [===========>..................] - ETA: 11:00 - loss: 0.3368 - acc: 0.8533WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-97d975b87113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(train_generator(), validation_data=valid_generator(), callbacks=callbacks_list,\n\u001b[0;32m----> 5\u001b[0;31m           steps_per_epoch = 101681, validation_steps = 25421, epochs = 150)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dataweekends/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath=\"skip_gram_weights/skip_gram_model01.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(train_generator(), validation_data=valid_generator(), callbacks=callbacks_list,\n",
    "          steps_per_epoch = 101681, validation_steps = 25421, epochs = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y) in train_generator():\n",
    "    print(x)\n",
    "    print(model.predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_of_dept(dept):\n",
    "    return list(map(course_to_id.get, crs_df[crs_df['CRS_SUBJ_DEPT_CD'] == dept]['agg_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.layers[2].get_weights()[0]\n",
    "X = embedding_matrix\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X)\n",
    "\n",
    "X_trans = pca.transform(X)\n",
    "\n",
    "plt.scatter(X_trans[:,0],X_trans[:,1])\n",
    "x_dept = X_trans[get_ids_of_dept('MATH')]\n",
    "plt.scatter(x_dept[:,0],x_dept[:,1], c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_attr = pd.read_excel('/Users/rsciagli/documents/Fall2020/BAR/student_attribute_table_full.xlsx')\n",
    "# student_attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
